{"name":"Cerebro","tagline":"Never wake up at 3AM because an instance went down ever again.  ","body":"## Overview\r\n\r\nCerebro is a job deployment and monitoring system created for the purpose of automating common system administration tasks, including but not limited to: job deployment, job growth (i.e. adding new nodes to a cluster) and machine maintenance (EC2 instance degradation, for example).   For example, deploy a cluster of 4 MongoDB nodes with a push of a button, and if, 3 weeks later, one of them somehow disappears on a Saturday at 2AM when all your sysadmins are out on the town finishing their last round of drinks before last call, a new instance will automatically be provisioned, Mongo deployed to the machine and booted, all without needing to force anybody to sober up and get to a console.\r\n\r\n## Project Status\r\nCerebro has mostly been developed by [me](https://github.com/ZachGoldberg) and with some help and brainstorming from [Ryan Borgeouix](https://github.com/BlueDragonX).  Cerebro is in use in at least one production environment and a number of development/testing environment and \"works\".  That said, it is sorely lacking in documentation and is probably not yet as easy as it will be to get a fully working stack up.  Patches, forks, bug reports. etc are all more than welcome on the project's [github](https://github.com/ZachGoldberg/Cerebro) page, or you can contact me directly at zach@zachgoldberg.com with any issues you may be having.\r\n\r\n## Capabilities\r\n\r\n * Monitor an individual process on a cloud hosted VM\r\n * Reboot the process when certain conditions (e.g. using too much RAM) are met/exceeded\r\n * Provide STDUOT/STDERR as well as process metadata and statistics via HTML (human readable) and JSON (machine readable) HTTP APIs.  No more SSHing into a machine and manually tailing log files.\r\n * Monitor and manage multiple \"jobs\" or processes per machine\r\n * Monitor many machines across a cluster\r\n * Provision new machines in various cloud environments (currently only EC2 is supported, but plugins for rackspace and others are in the works)\r\n * Customizable job deployment via python with a [Fabric](http://docs.fabfile.org/en/1.6/)-like API.  (i.e. codify the deployment of your custom processes so cerebro can do it in a repeatable manner).\r\n * Cluster management via a JSON API, with command line tools and wrappers\r\n * The ability to auto-detect if a machine goes bad / disappears, decommission it\r\n   and spin up an identical replacement and redeploy to it, without any admin intervention.\r\n * Provides an HTML interface at the cluster level which has:\r\n   - The ability to update jobs in place (aka update code)\r\n   - An overview of what jobs are running on what machines, and where\r\n   - Links to the STDOUT/STDERR of every process in every job on every machine, \r\n     across the cluster.\r\n   - Basic machine vitals for all machines, including ram/cpu usage per process\r\n     and total machine utilization.\r\n\r\n## Getting Started in 6 steps\r\n\r\n 0. Bundle your software into an easily deployable package (Using python buildout, or an egg, or a WAR file, or a .deb etc.  If you don't already do this it's not a bad idea to start now).  \r\n 1. Write a python \"deployment class\" (see docs below) for your package \r\n 2. Write a system-deployment configuration file which defines how many machines you want your code to run on and provides credentials to needed cloud APIs (VM provider, DNS provider etc.)\r\n 3. Easy install cerebrod on a management server and start the daemon with \"cerebrod\" (see cerebrod --help for more).\r\n 4. Easy install cerebrod on your development machine and run \"cerebro updatejobcfg\" on your local machine and pass it your configuration and the location of your clustersitter you've created in step 3.\r\n 5. Checkout the cerebro web UI (http://managementserver:30000/overview) and watch things happen.  You'll find the generated DNS names for your machines and lots more info there.\r\n\r\nAnd, if all goes well and you need to scale up, that's a simple 2 step process:\r\n 0. Update your job config file to require more machines. \r\n 1. Run \"updatejobcfg\" with your updated config file\r\n\r\n## Under the Hood\r\n\r\nCerebro is made up of three parts: Task Sitter a Machine Sitter and a Cluster Sitter\r\n\r\n### Task Sitter\r\nTask Sitter -- A harness to manage an arbitrary task or process.\r\n\r\nGoal: Instead of thinking about how many machines you need to run a process on\r\nthe task sitter's goal is to force the admin to think instead in terms of CPU\r\nand RAM, an to plan how much of each resource a process should use ahead of time.\r\n\r\nThe Task Sitter's job is to enforce the limits that the admin thinks a process\r\nshould obey.  It can handle the cases where a process disobeys these limits.\r\n\r\nTogether with a machine sitter a machine can be completely managed to run\r\nvarious tasks efficiently within the resource constraints of the machine.\r\n\r\nWith a cluster sitter an admin can define how many CPUs and how much RAM a particular\r\ntask can use and it can go to machines, look for available CPU and RAM where\r\nthe process fits and slot it in there.\r\n\r\nTask Sitter Details\r\n\r\n * Define constraints\r\n    * Should always be alive? (--ensure-alive)\r\n    * Fixed % of a CPU (--cpu)\r\n    * Fixed MB of RAM (--mem)\r\n    * Fixed lifetime (--time-limit)\r\n\r\n * Define runtime metadata\r\n    * User ID (--uid)\r\n    * Should the proc be restarted on violation? (--restart)\r\n    * Maximum # of reboots (--max-restarts)\r\n    * stdout / stderr directories (--std**-location)\r\n\r\n * Monitoring\r\n    * HTTP Based Monitor (--http-monitoring, --http-monitoring-port)\r\n\r\n### Machine Sitter\r\n\r\nMachine Sitter Details\r\n\r\n  * Monitor a set of TaskSitters\r\n  * Reboot TaskSitters if they fail (should never happen)\r\n  * Provide an API to add new tasks and start/stop tasks on a machine\r\n  * Provides central log access for all tasks\r\n\r\n### Cluster Sitter\r\n\r\nCluster Sitter Details\r\n\r\n  * Monitors a set of MachineSitters in a cluster\r\n  * Accepts 'Jobs' which define how many cpus/memory a particular task needs,\r\n finds or creates machines (and deploys machinesitters if necessary) and then\r\n activates the \"Jobs\" as tasks on each machine\r\n  * Provides a web UI to see where all your tasks are.\r\n  * Pulls in data and aggregates it from the cluster, to see task CPU\r\n usage, task rebooting behavior, machine performance data etc.\r\n  * Provides an abstract \"DeploymentRecipe\" class that you can fill out\r\n to have the clustersitter actually deploy your jobs automagically.\r\n  * Presently knows how to spinup/teardown AWS instances, though implementing\r\n other cloud providers should be pretty straightforward as there is a\r\n pretty minimal interface to the 'providers'.\r\n  * Assigns DNS names (presently only using the Dynect API) to machines\r\n    so they can automagically go live (see more in the 'How to do DNS' section)\r\n  * Decomissions machines if they fail, will spin up new ones as replacements.\r\n  * Supports the notion of linked jobs: When job A is linked to job B job A will be placed on\r\n    every and only the machines job A is placed on.  Job B will also be\r\n    rebooted whenever job A is updated.\r\n  * Keeps track of how many idle machiens you own, and can decomission them\r\n    to keep to a predefined limit.\r\n\r\n\r\n### Providers\r\n * Cerebro uses an interface for talking to both a cloud provider and a DNS provider\r\n * Presently only AWS/EC2 is implemented as a cloud-VM provider and Dynect is the only DNS providr\r\n * The interface is sufficiently minimal (aka create_instances() or dns_add_record()/dns_delete_record())\r\n    that it should be very simple to expand to other providers (linode, rackspace etc.)\r\n\r\n## Configuration\r\n-------------\r\n\r\nCerebro Configuration File:\r\n # See settings.py\r\n\r\nExample Job Configuration Format\r\n\r\n    {\r\n        \"dns_basename\": \"redis.startup.com\",\r\n        \"deployment_recipe\": \"mystartup.recipes.deploy\",\r\n        \"deployment_layout\": {\r\n            \"aws-us-west-2a\": {\r\n                \"mem\": 500,\r\n                \"cpu\": 1\r\n            },\r\n            \"aws-us-east-1b\": {\r\n                \"mem\": 50,\r\n                \"cpu\": 10\r\n            }\r\n        },\r\n        \"recipe_options\": {\r\n            # Passed as a dictionary to your jobs\r\n            \"release_dir\": \"/opt/startup/releases/\"\r\n        },\r\n        \"persistent\": true,\r\n        \"task_configuration\":\r\n            {\r\n                # Tasksitter configuration. \r\n                \"allow_exit\": false,\r\n                \"name\": \"Portal Server\",\r\n                \"command\": \"/opt/code/run_portal_server\",\r\n                \"auto_start\": false,\r\n                \"ensure_alive\": true,\r\n                \"max_restarts\": -1,\r\n                \"restart\": true,\r\n                \"uid\": 0,\r\n\t\t\"cpu\": .5, # allow this job to use 50% of CPU\r\n\t\t\"mem\": 1200, # Allow this job to use 1.2GB of RAM\r\n            }\r\n    },\r\n\r\n\r\n### Deployment Recipe Interface\r\n\r\n    def run_deploy(options):\r\n        # API?\r\n        logger.*()\r\n\r\n### DNS Setup\r\n\r\n  *  In the job configuration format there is a field called \"dns_basename\"\r\n  *  This should be set to something like \"myjobname.mydomain.com\" e.g. \"redis.startup.com\"\r\n  *  Cerebro will then add two new records underneath that name for each machine.  It will\r\n\r\n      1. Create #.PROVIDER_REGION.basename as a A record to the machine\r\n      2. Add another CNAME to PROVIDER_REGION.basename to #.PROVIDER_REGION etc.\r\n\r\n  * You should manually setup, e.g. \"redis.startup.com\" to be a cname to all of the PROVIDER_REGION.redis.startup.com.  A complete DNS layout looks as follows\r\n\r\n        startup.com\r\n        redis.startup.com (Admin Created)\r\n           -> CNAME aws-us-west-1.redis.startup.com (Admin Created)\r\n           -> CNAME aws-us-east-1.redis.startup.com (Admin Created)\r\n\r\n        aws-us-west-1.redis.startup.com (Admin Created)\r\n           -> A 45.67.20.106 (Cerebro Created)\r\n           -> A 45.67.20.105 (Cerebro Created)\r\n \r\n        0.aws-uswest-1.redis.startup.com (Cerebro Created)\r\n           -> A 45.67.20.106 (Cerebro Created)\r\n        1.aws-uswest-1.redis.startup.com (Cerebro Created)\r\n           -> A 45.67.20.105 (Cerebro Created)\r\n\r\n        aws-us-east-1.redis.startup.com (Admin Created)\r\n           -> A 12.67.20.106 (Cerebro Created)\r\n \r\n        0.aws-us-east-1.redis.startup.com (Cerebro Created)\r\n           -> A 12.67.20.106 (Cerebro Created)\r\n\r\n\r\nSo, if you point your servers to redis.startup.com they should get either \r\n\r\n  1. If your using global load balancing, a cname to one of aws-us-west-1.redis.startup.com or \r\n      aws-us-east-1.redis.startup.com based on the callers location\r\n  2. or both CNAMEs\r\n\r\nThe cname returns an A record for each machine of that type.  e.g. redis.startup.com -> aws-us-west-1.redis.startup.com -> 12.67.20.106\r\n\r\n## Security\r\nI've had a few questions on Cerebro's security model.  Namely, that there is none.   This is for two reasons: time, and it's not immediately obvious to me that one is required.  Your cloud should, in an ideal world, be completely firewalled off from the outside world.  All of cerebro's management is done via TCP connections on non-standard ports which should be accessible only within your firewalled cloud or VPC.  To manage my machines within this environment I usually poke a hole or two with a reverse SSH port forward, or simply VPN beyond the firewall.  This isn't a perfect scenario, anybody within your cloud can do some bad things, but it seems 'good enough' until somebody cares enough to beef up the internal security model.","google":"UA-42767602-1","note":"Don't delete this file! It's used internally to help with page regeneration."}